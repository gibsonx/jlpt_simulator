{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b011bdf967ca7ff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Language Knowledge (Vocabulary)\n",
    "Duration: 30 minutes\n",
    "Content: This section tests your knowledge of Japanese vocabulary, including kanji readings, orthography, word formation, contextually-defined expressions, paraphrases, and usage\n",
    "It mainly composes following five categories:\n",
    "- ``Reading Kana`` (Pronunciation Questions): Given a kanji word, choose the correct kana reading.\n",
    "- `Writing Kanji` (Writing Questions): Given a word written in kana, choose the correct kanji representation.\n",
    "- `Word Meaning` Selection (Vocabulary Understanding): Choose the most suitable word to fill in the sentence from four options.\n",
    "- `Synonym Replacement`: Select a word that has the same or similar meaning as the underlined word.\n",
    "- `Vocabulary Usage`: Assess the usage of words in actual contexts, choosing the most appropriate word usage, including some common Japanese expressions or fixed phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f526a68a7e73a65",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T15:00:09.085874900Z",
     "start_time": "2025-05-11T15:00:09.069823400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import uuid\n",
    "import threading\n",
    "import asyncio\n",
    "from typing import *\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import yaml\n",
    "import sys\n",
    "import asyncio\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from graphs.common.utils import collect_vocabulary\n",
    "from langchain_openai import AzureOpenAI,AzureChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_community.embeddings import XinferenceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from libs.LLMs import *\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage,RemoveMessage,HumanMessage,AIMessage,ToolMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Optional\n",
    "\n",
    "if sys.platform.startswith(\"win\"):\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
    "\n",
    "from ExamTaskHandler import ExamTaskHandler\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "initial_id",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T15:00:09.121238700Z",
     "start_time": "2025-05-11T15:00:09.076863Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import N3 Vocabulary\n",
    "file_path = '../../Vocab/n3.csv'\n",
    "# Display the content of the CSV file\n",
    "vocab_dict = collect_vocabulary(file_path)\n",
    "with open(\"../../Vocab/topics.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    topics_list = [line.strip() for line in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1cdce8b1f57ef3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### load Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c63b54b5c2a36",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Exam Paper Outline\n",
    "### A. overall thinking the structure of an exam\n",
    "1. distribution of the difficulty \n",
    "2. topics\n",
    "3. reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e339295a858fb5f8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T15:00:09.122243400Z",
     "start_time": "2025-05-11T15:00:09.107931800Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# instruction = \"\"\"\n",
    "# Section 1: Vocabulary and Grammar\n",
    "# - Kanji reading (kanji_reading): 8 questions\n",
    "# - Write Chinese characters (write_chinese): 6 questions\n",
    "# - Word Meaning Selection (word_meaning): 11 questions\n",
    "# - Synonyms substitution (synonym_substitution): 5 questions\n",
    "# - word usage (word_usage): 5 questions\n",
    "# - Grammar fill in the blank (sentence_grammar): 13 questions\n",
    "# - Sentence sorting (sentence_sort): 5 questions\n",
    "# - Grammar structure selection (sentence_structure): 4-5 questions\n",
    "# \n",
    "# Section 2: Reading Comprehension\n",
    "# - Short passages (short_passage_read): 4 articles\n",
    "# - Mid-size passages (midsize_passage_read): 2 articles\n",
    "# - Long passages (long_passage_read): 1 articles\n",
    "# - Information retrieval (info_retrieval): 1 articles\n",
    "# \n",
    "# Section 3: Listening Comprehension\n",
    "# - Topic understanding (topic_understanding): 6 questions\n",
    "# - Key understanding (keypoint_understanding): 6 questions\n",
    "# - Summary understanding (summary_understanding): 3 questions\n",
    "# - Active expression (active_expression): 4 questions\n",
    "# - Immediate acknowledgment (immediate_ack): 9 questions\n",
    "# \"\"\"\n",
    "\n",
    "instruction = \"\"\"\n",
    "Section 1: Vocabulary and Grammar\n",
    "- Kanji reading (kanji_reading): 1 question\n",
    "- Write Chinese characters (write_chinese): 1 question\n",
    "- Word Meaning Selection (word_meaning): 1 question\n",
    "- Synonyms substitution (synonym_substitution): 1 question\n",
    "- word usage (word_usage): 1 question\n",
    "- Grammar fill in the blank (sentence_grammar): 1 question\n",
    "- Sentence sorting (sentence_sort): 1 question\n",
    "- Grammar structure selection (sentence_structure): 1 question\n",
    "\n",
    "Section 2: Reading Comprehension\n",
    "- Short passages (short_passage_read): 1 articles\n",
    "- Mid-size passages (midsize_passage_read): 1 articles\n",
    "- Long passages (long_passage_read): 1 articles\n",
    "- Information retrieval (info_retrieval): 1 articles\n",
    "\n",
    "Section 3: Listening Comprehension\n",
    "- Topic understanding (topic_understanding): 1 questions\n",
    "- Key understanding (keypoint_understanding): 1 questions\n",
    "- Summary understanding (summary_understanding): 1 questions\n",
    "- Active expression (active_expression): 1 questions\n",
    "- Immediate acknowledgment (immediate_ack): 1 questions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "direct_gen_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            f\"You are a japanese teacher. Your job is to write an outline for a JLPT Japanese-Language Proficiency Test N3 level exam paper. the complexity should be restricted to N3 level and respect japanese culture. The JLPT Japanese-Language Proficiency Test exam paper includes a mix of easy, moderate, and difficult questions to accurately assess the test-taker's proficiency across different aspects of the language.\"\n",
    "            f\"First,  randomly pick words in 'Vocabulary' for questions in Vocabulary and Grammar. At Section 2 and 3, randomly choose topics in 'TopicList' for Reading Comprehension and Listening Comprehension Sections, don't repeat to choose a same word or topic\"  \n",
    "            f\"Second, you should abide by the provided exam instruction and decide the number of questions and content in the each Section.\"\n",
    "            f\"Finally, write the outline of the examination paper in japanese and provide question topics according to the instructions.\"\n",
    "            f\"Instruction: {instruction}\",\n",
    "        ),\n",
    "        (\"user\", \"TopicList: {topic_list}, Vocabulary: {vocab_dict}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5f032dadafac1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Strcuture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39329a71b16d2400",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T15:00:09.123242900Z",
     "start_time": "2025-05-11T15:00:09.112281Z"
    }
   },
   "outputs": [],
   "source": [
    "class QuestionTopic(BaseModel):\n",
    "    topic: str = Field(..., title=\"a vocabulary or topic hint for a question\")\n",
    "    \n",
    "    \n",
    "class Subsection(BaseModel):\n",
    "    subsection_title: str = Field(..., title=\"subsection English word in () only from the instruction\")\n",
    "    description: str = Field(..., title=\"giving the number of questions and requirements\")\n",
    "    question_topics: Optional[List[QuestionTopic]] = Field(\n",
    "        default_factory=list\n",
    "    )\n",
    "    \n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        question_topics_str = \"\\n\".join(\n",
    "            f\"- **{qt.topic}**\" for qt in self.question_topics\n",
    "        )\n",
    "        return f\"### {self.subsection_title}\\n\\n{self.description}\\n\\n{question_topics_str}\".strip()\n",
    "\n",
    "class Section(BaseModel):\n",
    "    section_title: str = Field(..., title=\"Title of the section\")\n",
    "    subsections: Optional[List[Subsection]] = Field(\n",
    "        default_factory=list,\n",
    "        title=\"Titles and reason for each subsection of the JLPT exam page.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            subsection.as_str for subsection in self.subsections or []\n",
    "        )\n",
    "        return f\"## {self.section_title}\\n\\n{subsections}\".strip()\n",
    "\n",
    "\n",
    "class Outline(BaseModel):\n",
    "    page_title: str = Field(..., title=\"Title of the JLPT exam page\")\n",
    "    sections: List[Section] = Field(\n",
    "        default_factory=list,\n",
    "        title=\"Titles and descriptions for each section of the JLPT exam paper.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        sections = \"\\n\\n\".join(section.as_str for section in self.sections)\n",
    "        return f\"# {self.page_title}\\n\\n{sections}\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Read the topics from a file, sort them, and print the sorted list  \n",
    "def process_topics(file_path):\n",
    "    try:  \n",
    "        # Read the file  \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:  \n",
    "            topics = file.readlines()  \n",
    "          \n",
    "        # Remove any extra whitespace or newline characters  \n",
    "        topics = [topic.strip() for topic in topics if topic.strip()]  \n",
    "          \n",
    "        # Shuffle the topics randomly  \n",
    "        random.shuffle(topics)  \n",
    "                \n",
    "    except FileNotFoundError:  \n",
    "        print(\"The file was not found. Please check the file path.\")  \n",
    "    except Exception as e:  \n",
    "        print(\"An error occurred:\", str(e)) \n",
    "      \n",
    "    except FileNotFoundError:  \n",
    "        print(\"The file was not found. Please check the file path.\")  \n",
    "    except Exception as e:  \n",
    "        print(\"An error occurred:\", str(e)) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T15:00:09.132565700Z",
     "start_time": "2025-05-11T15:00:09.125243800Z"
    }
   },
   "id": "d0730de416980c6"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Preload all topics from the file\n",
    "with open(\"../../Vocab/topics.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    topics_list = [line.strip() for line in file]\n",
    "\n",
    "generate_outline_direct = direct_gen_outline_prompt | azure_llm.with_structured_output(Outline)\n",
    "initial_outline = generate_outline_direct.invoke({\"topic_list\": topics_list, \"vocab_dict\": vocab_dict})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T15:00:18.403441400Z",
     "start_time": "2025-05-11T15:00:09.131564700Z"
    }
   },
   "id": "8fc17fee14c40716"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# JLPT N3 レベル試験問題\n\n## 語彙と文法\n\n### Kanji reading (kanji_reading)\n\n1問。与えられた漢字の読みを選択する。\n\n- **郵便**\n\n### Write Chinese characters (write_chinese)\n\n1問。与えられた言葉の漢字表記を書く。\n\n- **以前**\n\n### Word Meaning Selection (word_meaning)\n\n1問。与えられた言葉の意味を選択する。\n\n- **泳ぎ**\n\n### Synonyms substitution (synonym_substitution)\n\n1問。与えられた言葉と同じ意味の言葉を選択する。\n\n- **語学**\n\n### word usage (word_usage)\n\n1問。与えられた言葉を正しい文脈で使用する。\n\n- **愛する**\n\n### Grammar fill in the blank (sentence_grammar)\n\n1問。与えられた文の空欄を正しい文法で埋める。\n\n- **語る**\n\n### Sentence sorting (sentence_sort)\n\n1問。与えられた文の順序を正しい形に並べる。\n\n- **率**\n\n### Grammar structure selection (sentence_structure)\n\n1問。与えられた文に適切な文法構造を選択する。\n\n- **拭く**\n\n## 読解\n\n### Short passages (short_passage_read)\n\n1問。短い文章を読んで質問に答える。\n\n- **店で価格を尋ねる**\n\n### Mid-size passages (midsize_passage_read)\n\n1問。中程度の長さの文章を読んで質問に答える。\n\n- **購入したい商品の説明**\n\n### Long passages (long_passage_read)\n\n1問。長い文章を読んで質問に答える。\n\n- **割引交渉**\n\n### Information retrieval (info_retrieval)\n\n1問。文章から情報を見つけ出す。\n\n- **レストランで食べ物を注文する**\n\n## 聴解\n\n### Topic understanding (topic_understanding)\n\n1問。短い会話を聞いてその話題を理解する。\n\n- **食事の好みについて話す**\n\n### Key understanding (keypoint_understanding)\n\n1問。会話を聞いて重要な情報を理解する。\n\n- **料理を褒める**\n\n### Summary understanding (summary_understanding)\n\n1問。会話の要約を理解する。\n\n- **道を尋ねる**\n\n### Active expression (active_expression)\n\n1問。会話の中で適切な応答を選択する。\n\n- **交通手段について話す**\n\n### Immediate acknowledgment (immediate_ack)\n\n1問。会話を聞いて即座に理解する。\n\n- **交通状況について話す**"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown(initial_outline.as_str))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T15:00:18.408220100Z",
     "start_time": "2025-05-11T15:00:18.405174600Z"
    }
   },
   "id": "d53b7d9ff86a5377"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 語彙と文法:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Processing Kanji reading (kanji_reading):   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---WEB SEARCH---\n",
      "---Generator----\n",
      "---REVISOR---\n",
      "--- AI Reviser feels Good Enough ---\n",
      "--- Formatter ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Kanji reading (kanji_reading): 100%|██████████| 1/1 [00:17<00:00, 17.42s/it]\u001B[A\n",
      "Processing 語彙と文法:  12%|█▎        | 1/8 [00:17<02:01, 17.42s/it]\n",
      "Processing Write Chinese characters (write_chinese):   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---WEB SEARCH---\n",
      "---Generator----\n",
      "---REVISOR---\n",
      "--- AI Reviser feels Good Enough ---\n",
      "--- Formatter ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Write Chinese characters (write_chinese): 100%|██████████| 1/1 [00:14<00:00, 14.39s/it]\u001B[A\n",
      "Processing 語彙と文法:  25%|██▌       | 2/8 [00:31<01:33, 15.64s/it]\n",
      "Processing Word Meaning Selection (word_meaning):   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---WEB SEARCH---\n",
      "---Generator----\n",
      "---REVISOR---\n",
      "--- AI Reviser feels Good Enough ---\n",
      "--- Formatter ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Word Meaning Selection (word_meaning): 100%|██████████| 1/1 [00:46<00:00, 46.79s/it]\u001B[A\n",
      "Processing 語彙と文法:  38%|███▊      | 3/8 [01:18<02:29, 29.86s/it]\n",
      "Processing Synonyms substitution (synonym_substitution):   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---WEB SEARCH---\n",
      "---Generator----\n",
      "---REVISOR---\n",
      "--- AI Reviser feels Good Enough ---\n",
      "--- Formatter ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Synonyms substitution (synonym_substitution): 100%|██████████| 1/1 [00:16<00:00, 16.45s/it]\u001B[A\n",
      "Processing 語彙と文法:  50%|█████     | 4/8 [01:35<01:38, 24.57s/it]\n",
      "Processing word usage (word_usage):   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---WEB SEARCH---\n",
      "---Generator----\n",
      "---REVISOR---\n",
      "--- AI Reviser feels Good Enough ---\n",
      "--- Formatter ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing word usage (word_usage): 100%|██████████| 1/1 [00:52<00:00, 52.98s/it]\u001B[A\n",
      "Processing 語彙と文法:  62%|██████▎   | 5/8 [02:28<01:44, 34.81s/it]\n",
      "Processing Grammar fill in the blank (sentence_grammar):   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---WEB SEARCH---\n",
      "---Generator----\n",
      "---REVISOR---\n",
      "--- AI Reviser feels Good Enough ---\n",
      "--- Formatter ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Grammar fill in the blank (sentence_grammar): 100%|██████████| 1/1 [00:16<00:00, 16.80s/it]\u001B[A\n",
      "Processing 語彙と文法:  75%|███████▌  | 6/8 [02:44<00:57, 28.69s/it]\n",
      "Processing Sentence sorting (sentence_sort):   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---WEB SEARCH---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding a node to a graph that has already been compiled. This will not be reflected in the compiled graph.\n",
      "\n",
      "Processing Sentence sorting (sentence_sort): 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\u001B[A\n",
      "Processing 語彙と文法:  88%|████████▊ | 7/8 [02:46<00:19, 19.87s/it]\n",
      "Processing Grammar structure selection (sentence_structure):   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---WEB SEARCH---\n",
      "---Generator----\n",
      "---REVISOR---\n",
      "--- AI Reviser feels Good Enough ---\n",
      "--- Formatter ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Grammar structure selection (sentence_structure): 100%|██████████| 1/1 [00:27<00:00, 27.50s/it]\u001B[A\n",
      "Processing 語彙と文法: 100%|██████████| 8/8 [03:14<00:00, 24.26s/it]\n",
      "Processing 読解:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Processing Short passages (short_passage_read):   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---WEB SEARCH---\n",
      "---Generator----\n"
     ]
    }
   ],
   "source": [
    "outliner_json = initial_outline.model_dump_json()\n",
    "data = json.loads(outliner_json)  # Replace with your actual JSON data\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "output_data = {'sections': []}\n",
    "\n",
    "for section in data['sections']:\n",
    "    output_section = {'section_title': section['section_title'], 'subsections': []}\n",
    "\n",
    "    for subsection in tqdm(section['subsections'], desc=f\"Processing {section['section_title']}\"):\n",
    "        function_name = subsection['subsection_title'].split(' (')[1].rstrip(')')\n",
    "        questions = subsection['question_topics']\n",
    "\n",
    "        for question in tqdm(questions, desc=f\"Processing {subsection['subsection_title']}\"):\n",
    "            handler = ExamTaskHandler(vocab=vocab_dict)\n",
    "            func = getattr(handler, function_name, None)\n",
    "\n",
    "            if func:\n",
    "                max_attempts = 2\n",
    "                original_topic = question['topic']  # Optional: Track original topic\n",
    "                for attempt in range(max_attempts):\n",
    "                    try:\n",
    "                        result = json.loads(func(question['topic']))\n",
    "                        question['result'] = result\n",
    "                        break  # Exit on success\n",
    "                    except Exception as e:\n",
    "                        if attempt < max_attempts - 1:\n",
    "                            # Replace topic and retry\n",
    "                            question['topic'] = random.choice(topics_list)\n",
    "                        else:\n",
    "                            question['result'] = f\"Error after {max_attempts} attempts: {str(e)}\"\n",
    "            else:\n",
    "                question['result'] = f\"Method {function_name} not found\"\n",
    "\n",
    "        output_subsection = {\n",
    "            'subsection_title': subsection['subsection_title'],\n",
    "            'description': subsection['description'],\n",
    "            'question_topics': questions\n",
    "        }\n",
    "        output_section['subsections'].append(output_subsection)\n",
    "\n",
    "    output_data['sections'].append(output_section)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the total execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Total execution time: {execution_time:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-05-11T15:00:18.409595400Z"
    }
   },
   "id": "5c2b7e675e5302a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "yaml_str = yaml.dump(output_data, allow_unicode=True, sort_keys=False, default_flow_style=False)\n",
    "# Display as formatted YAML\n",
    "display(Markdown(yaml_str))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "43e411616ee939b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "307398e88bf20dc3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "aa98a476d1ded815"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
