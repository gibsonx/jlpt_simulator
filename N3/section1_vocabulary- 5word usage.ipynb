{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b011bdf967ca7ff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Language Knowledge (Vocabulary)\n",
    "Duration: 30 minutes\n",
    "Content: This section tests your knowledge of Japanese vocabulary, including kanji readings, orthography, word formation, contextually-defined expressions, paraphrases, and usage\n",
    "It mainly composes following five categories:\n",
    "- ``Reading Kana`` (Pronunciation Questions): Given a kanji word, choose the correct kana reading.\n",
    "- `Writing Kanji` (Writing Questions): Given a word written in kana, choose the correct kanji representation.\n",
    "- `Word Meaning` Selection (Vocabulary Understanding): Choose the most suitable word to fill in the sentence from four options.\n",
    "- `Synonym Replacement`: Select a word that has the same or similar meaning as the underlined word.\n",
    "- `Vocabulary Usage`: Assess the usage of words in actual contexts, choosing the most appropriate word usage, including some common Japanese expressions or fixed phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f526a68a7e73a65",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-12T08:37:32.931602100Z",
     "start_time": "2025-04-12T08:37:31.002787900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import uuid\n",
    "from typing import *\n",
    "from langchain_openai import AzureOpenAI,AzureChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_community.embeddings import XinferenceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage,RemoveMessage,HumanMessage,AIMessage,ToolMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Optional\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-12T08:37:32.946738Z",
     "start_time": "2025-04-12T08:37:32.932608400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  expression reading\n0         身長   しんちょう\n1          隊      たい\n2         繋ぐ     つなぐ\n3          仏     ほとけ\n4         現実    げんじつ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>expression</th>\n      <th>reading</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>身長</td>\n      <td>しんちょう</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>隊</td>\n      <td>たい</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>繋ぐ</td>\n      <td>つなぐ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>仏</td>\n      <td>ほとけ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>現実</td>\n      <td>げんじつ</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import N3 Vocabulary\n",
    "file_path = '../Vocab/n3.csv'\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "words = data.iloc[:, :2].sample(frac=1).reset_index(drop=True)\n",
    "# Display the content of the CSV file\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1cdce8b1f57ef3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "895a64c8ba51e354",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-12T08:37:32.952824400Z",
     "start_time": "2025-04-12T08:37:32.946738Z"
    }
   },
   "outputs": [],
   "source": [
    "# azure_llm = AzureChatOpenAI(\n",
    "#     azure_endpoint=\"https://tooldev-openai.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview\",\n",
    "#     api_key=os.environ[\"AZURE_API_KEY\"],\n",
    "#     model_name=\"gpt-4o\",\n",
    "#     api_version=\"2025-01-01-preview\",\n",
    "#     temperature=0.5,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5643fdb248144100",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-12T08:37:33.063820900Z",
     "start_time": "2025-04-12T08:37:32.949813400Z"
    }
   },
   "outputs": [],
   "source": [
    "aws_llm = ChatBedrock(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "     # model_id=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    model_kwargs=dict(temperature=0.5),\n",
    "    region = \"us-east-2\",\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5f032dadafac1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Question Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795bb3d95c0cc50",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Kanji 读假名（读音问题）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a966afd733d860d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-12T08:37:33.068271700Z",
     "start_time": "2025-04-12T08:37:33.040351100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     expression reading\n",
      "0            身長   しんちょう\n",
      "1             隊      たい\n",
      "2            繋ぐ     つなぐ\n",
      "3             仏     ほとけ\n",
      "4            現実    げんじつ\n",
      "...         ...     ...\n",
      "2134         切れ      きれ\n",
      "2135         討つ      うつ\n",
      "2136         場面     ばめん\n",
      "2137         変化     へんか\n",
      "2138         警告    けいこく\n",
      "\n",
      "[2139 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "# Graph state\n",
    "class QuestionState(TypedDict):\n",
    "    topic: str\n",
    "    question: str\n",
    "    improved_question: str\n",
    "    final_question: str\n",
    "\n",
    "\n",
    "kanji_example = \"\"\"\n",
    "問題5\n",
    "\n",
    "つぎのことばの使い方として最もよいものを、1・2・3・4から一つえらびなさい。\n",
    "\n",
    "31. 内容\n",
    "\t1.\t修理のため、エアコンの内容を一度取り出します\n",
    "\t2.\t鍋の中にカレーの内容を入れて、1時間くらい煮てください\n",
    "\t3.\t古い財布から新しい財布へ内容を移しました\n",
    "\t4.\tこの手紙の内容は、ほかの人には秘密にしてください\n",
    "\n",
    "32. 活動\n",
    "\t1.\t彼は有名なロック歌手だったが、今は活動していない\n",
    "\t2.\t山に登ると、新鮮な空気が活動していて気持ちがいい\n",
    "\t3.\tさっきまで活動していたパソコンが、急に動かなくなった\n",
    "\t4.\t駅前のコンビニは24時間活動しているので便利だ\n",
    "\n",
    "33. 落ち着く\n",
    "\t1.\tこの辺りは、冬になると雪が落ち着いて、春になるまで溶けません\n",
    "\t2.\tシャツにしみが落ち着いてしまって、洗ってもきれいになりません\n",
    "\t3.\tあそこの木の上に美しい鳥が落ち着いています\n",
    "\t4.\t大好きなこの曲を聞くと、いつも気持ちが落ち着きます\n",
    "\n",
    "34. ぐっすり\n",
    "\t1.\t遠慮しないで、ぐっすり食べてください\n",
    "\t2.\t優勝できたのは、毎日ぐっすり練習したからだと思う\n",
    "\t3.\t今日は疲れているので、朝までぐっすり眠れそうだ\n",
    "\t4.\t古い友人と久しぶりに会って、ぐっすりおしゃべりした\n",
    "\n",
    "35. 性格\n",
    "\t1.\t日本の古い性格に興味があるので、神社やお寺によく行きます\n",
    "\t2.\t森さんはおとなしい性格で、自分の意見はあまり言いません\n",
    "\t3.\t値段が高くても、塗装で性格のいい車を買うつもりです\n",
    "\t4.\t音楽の性格を伸ばすために、5歳から専門家の指導を受けました\n",
    "\"\"\"\n",
    "\n",
    "print(words)\n",
    "\n",
    "# Nodes\n",
    "def generate_question(state: QuestionState):\n",
    "    \"\"\"First LLM call to generate initial question\"\"\"\n",
    "\n",
    "    msg = aws_llm.invoke(f\"\"\"You are a Japanese teacher. Your job is to write 5 word usage for candidates from the following sentences, select the most appropriate usage of the word from options 1, 2, 3, and 4. (Includes Japanese idiomatic expressions and fixed collocations.)\n",
    "\"\"\"\n",
    "            f\"The vocabulary should be restricted to N3 level, you can refer to the vocabulary in the word list, choosing random words for the questions\"\n",
    "            f\"please refer the question examples following the formal exam paper\"\n",
    "            f\"append the correct answer and explanation of main challenges and why teacher asks this question to candidate in chinese at each question\"\n",
    "            f\"Finally, beautify markdown format\"\n",
    "            f\"topic list: {state['topic']}\"\n",
    "            f\"word list: {words}\"\n",
    "            f\"formal exam paper: {kanji_example}\")\n",
    "    \n",
    "    return {\"question\": msg.content}\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "kanji_workflow = StateGraph(QuestionState)\n",
    "\n",
    "# Add nodes\n",
    "kanji_workflow.add_node(\"generate_question\", generate_question)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "kanji_workflow.add_edge(START, \"generate_question\")\n",
    "kanji_workflow.add_edge(\"generate_question\", END)\n",
    "\n",
    "# Compile\n",
    "kanji_graph = kanji_workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec483e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T08:37:33.068271700Z",
     "start_time": "2025-04-12T08:37:33.055237200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show workflow\n",
    "#display(Image(kanji_graph.get_graph().draw_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c2e1e740fbe6971",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-12T08:37:34.724688500Z",
     "start_time": "2025-04-12T08:37:33.056792200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error raised by bedrock service\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langchain_aws\\llms\\bedrock.py\", line 956, in _prepare_input_and_invoke\n",
      "    response = self.client.invoke_model(**request_options)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\botocore\\client.py\", line 570, in _api_call\n",
      "    return self._make_api_call(operation_name, kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\botocore\\context.py\", line 124, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Administrator\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\botocore\\client.py\", line 1031, in _make_api_call\n",
      "    raise error_class(parsed_response, operation_name)\n",
      "botocore.exceptions.ClientError: An error occurred (UnrecognizedClientException) when calling the InvokeModel operation: The security token included in the request is invalid.\n",
      "Error in LangChainTracer.on_llm_error callback: TracerException('No indexed run ID 482e3c03-c68c-48ee-9541-d3a577671267.')\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (UnrecognizedClientException) when calling the InvokeModel operation: The security token included in the request is invalid.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mClientError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Invoke\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m kanji = \u001B[43mkanji_graph\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtopic\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m店で商品の特徴や価格を尋ねる | 電車やバスの乗り方を尋ねる | 今日の天気や気温について話す| 映画やドラマの感想を話す | 仕事のスケジュールや業務内容を話す\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m display(Markdown(kanji[\u001B[33m\"\u001B[39m\u001B[33mquestion\u001B[39m\u001B[33m\"\u001B[39m]))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2375\u001B[39m, in \u001B[36mPregel.invoke\u001B[39m\u001B[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001B[39m\n\u001B[32m   2373\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2374\u001B[39m     chunks = []\n\u001B[32m-> \u001B[39m\u001B[32m2375\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2376\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   2377\u001B[39m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2378\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2379\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_keys\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2380\u001B[39m \u001B[43m    \u001B[49m\u001B[43minterrupt_before\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterrupt_before\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2381\u001B[39m \u001B[43m    \u001B[49m\u001B[43minterrupt_after\u001B[49m\u001B[43m=\u001B[49m\u001B[43minterrupt_after\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2382\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2383\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2384\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   2385\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_mode\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mvalues\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\n\u001B[32m   2386\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlatest\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2031\u001B[39m, in \u001B[36mPregel.stream\u001B[39m\u001B[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001B[39m\n\u001B[32m   2025\u001B[39m     \u001B[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001B[39;00m\n\u001B[32m   2026\u001B[39m     \u001B[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001B[39;00m\n\u001B[32m   2027\u001B[39m     \u001B[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001B[39;00m\n\u001B[32m   2028\u001B[39m     \u001B[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001B[39;00m\n\u001B[32m   2029\u001B[39m     \u001B[38;5;66;03m# with channel updates applied only at the transition between steps.\u001B[39;00m\n\u001B[32m   2030\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m loop.tick(input_keys=\u001B[38;5;28mself\u001B[39m.input_channels):\n\u001B[32m-> \u001B[39m\u001B[32m2031\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrunner\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2032\u001B[39m \u001B[43m            \u001B[49m\u001B[43mloop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2033\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstep_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2034\u001B[39m \u001B[43m            \u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2035\u001B[39m \u001B[43m            \u001B[49m\u001B[43mget_waiter\u001B[49m\u001B[43m=\u001B[49m\u001B[43mget_waiter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2036\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   2037\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# emit output\u001B[39;49;00m\n\u001B[32m   2038\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield from\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2039\u001B[39m \u001B[38;5;66;03m# emit output\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:230\u001B[39m, in \u001B[36mPregelRunner.tick\u001B[39m\u001B[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001B[39m\n\u001B[32m    228\u001B[39m t = tasks[\u001B[32m0\u001B[39m]\n\u001B[32m    229\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m230\u001B[39m     \u001B[43mrun_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    231\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    232\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    233\u001B[39m \u001B[43m        \u001B[49m\u001B[43mconfigurable\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m    234\u001B[39m \u001B[43m            \u001B[49m\u001B[43mCONFIG_KEY_SEND\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwriter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    235\u001B[39m \u001B[43m            \u001B[49m\u001B[43mCONFIG_KEY_CALL\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcall\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    236\u001B[39m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    237\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    238\u001B[39m     \u001B[38;5;28mself\u001B[39m.commit(t, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001B[39m, in \u001B[36mrun_with_retry\u001B[39m\u001B[34m(task, retry_policy, configurable)\u001B[39m\n\u001B[32m     38\u001B[39m     task.writes.clear()\n\u001B[32m     39\u001B[39m     \u001B[38;5;66;03m# run the task\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtask\u001B[49m\u001B[43m.\u001B[49m\u001B[43mproc\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[43m.\u001B[49m\u001B[43minput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ParentCommand \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m     42\u001B[39m     ns: \u001B[38;5;28mstr\u001B[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:546\u001B[39m, in \u001B[36mRunnableSeq.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    542\u001B[39m config = patch_config(\n\u001B[32m    543\u001B[39m     config, callbacks=run_manager.get_child(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mseq:step:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;250m \u001B[39m+\u001B[38;5;250m \u001B[39m\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    544\u001B[39m )\n\u001B[32m    545\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m i == \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m546\u001B[39m     \u001B[38;5;28minput\u001B[39m = \u001B[43mstep\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    547\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    548\u001B[39m     \u001B[38;5;28minput\u001B[39m = step.invoke(\u001B[38;5;28minput\u001B[39m, config)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:310\u001B[39m, in \u001B[36mRunnableCallable.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    308\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    309\u001B[39m     context.run(_set_config_context, config)\n\u001B[32m--> \u001B[39m\u001B[32m310\u001B[39m     ret = \u001B[43mcontext\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    311\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, Runnable) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.recurse:\n\u001B[32m    312\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m ret.invoke(\u001B[38;5;28minput\u001B[39m, config)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 56\u001B[39m, in \u001B[36mgenerate_question\u001B[39m\u001B[34m(state)\u001B[39m\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_question\u001B[39m(state: QuestionState):\n\u001B[32m     54\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"First LLM call to generate initial question\"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m56\u001B[39m     msg = \u001B[43maws_llm\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\"\"\u001B[39;49m\u001B[33;43mYou are a Japanese teacher. Your job is to write 5 word usage for candidates from the following sentences, select the most appropriate usage of the word from options 1, 2, 3, and 4. (Includes Japanese idiomatic expressions and fixed collocations.)\u001B[39;49m\n\u001B[32m     57\u001B[39m \u001B[33;43m\"\"\"\u001B[39;49m\n\u001B[32m     58\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mThe vocabulary should be restricted to N3 level, you can refer to the vocabulary in the word list, choosing random words for the questions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     59\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mplease refer the question examples following the formal exam paper\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     60\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mappend the correct answer and explanation of main challenges and why teacher asks this question to candidate in chinese at each question\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     61\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mFinally, beautify markdown format\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     62\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtopic list: \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mstate\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtopic\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     63\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mword list: \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mwords\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     64\u001B[39m \u001B[43m            \u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mformal exam paper: \u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mkanji_example\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     66\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[33m\"\u001B[39m\u001B[33mquestion\u001B[39m\u001B[33m\"\u001B[39m: msg.content}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:307\u001B[39m, in \u001B[36mBaseChatModel.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    296\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    297\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    298\u001B[39m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[32m   (...)\u001B[39m\u001B[32m    302\u001B[39m     **kwargs: Any,\n\u001B[32m    303\u001B[39m ) -> BaseMessage:\n\u001B[32m    304\u001B[39m     config = ensure_config(config)\n\u001B[32m    305\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[32m    306\u001B[39m         ChatGeneration,\n\u001B[32m--> \u001B[39m\u001B[32m307\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    308\u001B[39m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    309\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    310\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    311\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    312\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    313\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_name\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    314\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    315\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    316\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m.generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m],\n\u001B[32m    317\u001B[39m     ).message\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:843\u001B[39m, in \u001B[36mBaseChatModel.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m    835\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m    836\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    837\u001B[39m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[32m   (...)\u001B[39m\u001B[32m    840\u001B[39m     **kwargs: Any,\n\u001B[32m    841\u001B[39m ) -> LLMResult:\n\u001B[32m    842\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m--> \u001B[39m\u001B[32m843\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:683\u001B[39m, in \u001B[36mBaseChatModel.generate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    680\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[32m    681\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    682\u001B[39m         results.append(\n\u001B[32m--> \u001B[39m\u001B[32m683\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    684\u001B[39m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    685\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    686\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    687\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    688\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    689\u001B[39m         )\n\u001B[32m    690\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    691\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:908\u001B[39m, in \u001B[36mBaseChatModel._generate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    906\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    907\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._generate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m908\u001B[39m         result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    909\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    910\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    911\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    912\u001B[39m         result = \u001B[38;5;28mself\u001B[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langchain_aws\\chat_models\\bedrock.py:721\u001B[39m, in \u001B[36mChatBedrock._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    718\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m stop:\n\u001B[32m    719\u001B[39m         params[\u001B[33m\"\u001B[39m\u001B[33mstop_sequences\u001B[39m\u001B[33m\"\u001B[39m] = stop\n\u001B[32m--> \u001B[39m\u001B[32m721\u001B[39m     completion, tool_calls, llm_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_prepare_input_and_invoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    722\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    723\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    724\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    725\u001B[39m \u001B[43m        \u001B[49m\u001B[43msystem\u001B[49m\u001B[43m=\u001B[49m\u001B[43msystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    726\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mformatted_messages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    727\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    728\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    729\u001B[39m \u001B[38;5;66;03m# usage metadata\u001B[39;00m\n\u001B[32m    730\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m usage := llm_output.get(\u001B[33m\"\u001B[39m\u001B[33musage\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langchain_aws\\llms\\bedrock.py:971\u001B[39m, in \u001B[36mBedrockBase._prepare_input_and_invoke\u001B[39m\u001B[34m(self, prompt, system, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    969\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    970\u001B[39m         run_manager.on_llm_error(e)\n\u001B[32m--> \u001B[39m\u001B[32m971\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[32m    973\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stop \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    974\u001B[39m     text = enforce_stop_tokens(text, stop)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\langchain_aws\\llms\\bedrock.py:956\u001B[39m, in \u001B[36mBedrockBase._prepare_input_and_invoke\u001B[39m\u001B[34m(self, prompt, system, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    954\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mRequest body sent to bedrock: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrequest_options\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    955\u001B[39m logger.info(\u001B[33m\"\u001B[39m\u001B[33mUsing Bedrock Invoke API to generate response\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m956\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrequest_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    958\u001B[39m (\n\u001B[32m    959\u001B[39m     text,\n\u001B[32m    960\u001B[39m     thinking,\n\u001B[32m   (...)\u001B[39m\u001B[32m    964\u001B[39m     stop_reason,\n\u001B[32m    965\u001B[39m ) = LLMInputOutputAdapter.prepare_output(provider, response).values()\n\u001B[32m    966\u001B[39m logger.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mResponse received from Bedrock: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\botocore\\client.py:570\u001B[39m, in \u001B[36mClientCreator._create_api_method.<locals>._api_call\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    566\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[32m    567\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpy_operation_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m() only accepts keyword arguments.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    568\u001B[39m     )\n\u001B[32m    569\u001B[39m \u001B[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m570\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_api_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\botocore\\context.py:124\u001B[39m, in \u001B[36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    122\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m hook:\n\u001B[32m    123\u001B[39m     hook()\n\u001B[32m--> \u001B[39m\u001B[32m124\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\pythonProject\\jlpt\\venv\\Lib\\site-packages\\botocore\\client.py:1031\u001B[39m, in \u001B[36mBaseClient._make_api_call\u001B[39m\u001B[34m(self, operation_name, api_params)\u001B[39m\n\u001B[32m   1027\u001B[39m     error_code = error_info.get(\u001B[33m\"\u001B[39m\u001B[33mQueryErrorCode\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m error_info.get(\n\u001B[32m   1028\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mCode\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1029\u001B[39m     )\n\u001B[32m   1030\u001B[39m     error_class = \u001B[38;5;28mself\u001B[39m.exceptions.from_code(error_code)\n\u001B[32m-> \u001B[39m\u001B[32m1031\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m error_class(parsed_response, operation_name)\n\u001B[32m   1032\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1033\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parsed_response\n",
      "\u001B[31mClientError\u001B[39m: An error occurred (UnrecognizedClientException) when calling the InvokeModel operation: The security token included in the request is invalid.",
      "During task with name 'generate_question' and id 'e373d54b-384c-81a3-ffaf-12c40ecb3e97'"
     ]
    }
   ],
   "source": [
    "# Invoke\n",
    "kanji = kanji_graph.invoke({\"topic\": \"店で商品の特徴や価格を尋ねる | 電車やバスの乗り方を尋ねる | 今日の天気や気温について話す| 映画やドラマの感想を話す | 仕事のスケジュールや業務内容を話す\"})\n",
    "display(Markdown(kanji[\"question\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553844c1e99d121c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-12T08:37:34.724688500Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c778d8e10606427f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-12T08:37:34.724688500Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
